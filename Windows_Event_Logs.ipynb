{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install google-colab-selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1pwXVzoVFej",
        "outputId": "3a386030-9467-4ed3-b8ea-de9cfc871bef"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-colab-selenium\n",
            "  Downloading google_colab_selenium-1.0.13-py3-none-any.whl (8.1 kB)\n",
            "Collecting selenium (from google-colab-selenium)\n",
            "  Downloading selenium-4.20.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium->google-colab-selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium->google-colab-selenium)\n",
            "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium->google-colab-selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium->google-colab-selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium->google-colab-selenium) (4.11.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium->google-colab-selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.2.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->google-colab-selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium->google-colab-selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium, google-colab-selenium\n",
            "Successfully installed google-colab-selenium-1.0.13 h11-0.14.0 outcome-1.3.0.post0 selenium-4.20.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "import google_colab_selenium as gs\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "# Base URL for relative links\n",
        "base_url = 'https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/'\n",
        "\n",
        "# Headers with a user agent\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Function to scrape event details\n",
        "def scrape_event_details(event_url):\n",
        "    response = requests.get(event_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract Operating System\n",
        "    os_label = soup.find('span', id='ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_lblOSorSourceLabel')\n",
        "    if os_label:\n",
        "        os_text = os_label.find_next('td').text.strip() if os_label.find_next('td') else 'Not available'\n",
        "        # Split the operating systems using regular expression\n",
        "        os_data = re.split(r'\\s+and\\s+', os_text)\n",
        "    else:\n",
        "        os_data = ['Not available']\n",
        "\n",
        "    # Extract Category and Subcategory\n",
        "    category_span = soup.find('span', id='ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_categories')\n",
        "    category_data = category_span.text.strip() if category_span else 'Not available'\n",
        "\n",
        "    # Extract Type\n",
        "    type_row = soup.find('tr', id='ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_trType')\n",
        "    type_data = type_row.find_all('td')[1].text.strip() if type_row and len(type_row.find_all('td')) > 1 else 'Not available'\n",
        "\n",
        "    # Extract Description\n",
        "    description = soup.find('ul').find_next_sibling('p')\n",
        "    description = description.text.strip() if description else 'No description available'\n",
        "\n",
        "    # Extract Event Example\n",
        "    event_example = soup.find('p', class_='EventExample')\n",
        "    event_example = event_example.text.strip() if event_example else 'No example available'\n",
        "\n",
        "    return os_data, category_data, type_data, description, event_example\n",
        "\n",
        "# Main scraping function\n",
        "def main():\n",
        "    # Initialize the Selenium webdriver (e.g., Chrome)\n",
        "    driver = gs.Chrome()\n",
        "\n",
        "    # Navigate to the webpage\n",
        "    driver.get('https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/default.aspx')\n",
        "\n",
        "    # Find the radio button by its ID and click it\n",
        "    radio_button = driver.find_element(By.ID, 'ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_whichEvents_0')\n",
        "    radio_button.click()\n",
        "\n",
        "    # Get the updated page source after clicking the radio button\n",
        "    updated_page_source = driver.page_source\n",
        "\n",
        "    # Parse the updated page source with BeautifulSoup\n",
        "    soup = BeautifulSoup(updated_page_source, 'html.parser')\n",
        "\n",
        "    table = soup.find('table', id='ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_GridView1')\n",
        "    rows = table.find_all('tr')[1:]  # Get all rows except the header\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # Check if detailed_events.csv exists and load the existing data\n",
        "    existing_data = []\n",
        "    if os.path.exists('detailed_events.csv'):\n",
        "        existing_df = pd.read_csv('detailed_events.csv')\n",
        "        existing_data = existing_df['Event_ID'].astype(str).tolist()  # Convert Event_ID to string\n",
        "\n",
        "    driver.close()\n",
        "\n",
        "    # Filter rows to only include those with event IDs not in existing_data\n",
        "    rows_to_process = [row for row in rows if row.find_all('td')[1].text.strip() not in existing_data]\n",
        "\n",
        "    # Create a new progress bar with the correct total\n",
        "    progress_bar = tqdm(total=len(rows_to_process), desc=\"Scraping rows\", dynamic_ncols=True)\n",
        "\n",
        "    for row in rows_to_process:\n",
        "        cols = row.find_all('td')\n",
        "        if cols:\n",
        "            source = cols[0].text.strip()\n",
        "            event_id = cols[1].text.strip()\n",
        "            event_summary = cols[2].text.strip()\n",
        "            event_url = base_url + cols[2].find('a')['href']\n",
        "            os_data, category_data, type_data, description, event_example = scrape_event_details(event_url)\n",
        "            data.append([source, event_id, event_summary, event_url, os_data, category_data, type_data, description, event_example])\n",
        "            time.sleep(random.randint(1, 10))  # Random sleep between 1 and 10 seconds\n",
        "            progress_bar.update(1)  # Update the progress bar by 1 for each processed row\n",
        "\n",
        "    progress_bar.close()  # Close the progress bar when done\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['Source', 'Event_ID', 'Event Summary', 'URL', 'Operating Systems', 'Category and Subcategory', 'Type', 'Description', 'Event Example'])\n",
        "\n",
        "    # Append the new data to the existing detailed_events.csv file\n",
        "    if os.path.exists('detailed_events.csv'):\n",
        "        existing_df = pd.read_csv('detailed_events.csv')\n",
        "        combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
        "        combined_df.to_csv('detailed_events.csv', index=False)\n",
        "    else:\n",
        "        df.to_csv('detailed_events.csv', index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "loM1iX-r7iWl",
        "outputId": "274be15c-8ef4-419a-d3ca-687f1764ffe9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"c9657017-141c-45d4-ade4-0033ecd4dca6-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"c9657017-141c-45d4-ade4-0033ecd4dca6-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"c9657017-141c-45d4-ade4-0033ecd4dca6-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"c9657017-141c-45d4-ade4-0033ecd4dca6-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Scraping rows:   0%|          | 0/152 [00:00<?, ?it/s]\u001b[A\n",
            "Scraping rows:   1%|          | 1/152 [00:01<03:47,  1.51s/it]\u001b[A\n",
            "Scraping rows:   1%|▏         | 2/152 [00:03<05:12,  2.08s/it]\u001b[A\n",
            "Scraping rows:   2%|▏         | 3/152 [00:07<06:49,  2.75s/it]\u001b[A\n",
            "Scraping rows:   3%|▎         | 4/152 [00:12<08:30,  3.45s/it]\u001b[A\n",
            "Scraping rows:   3%|▎         | 5/152 [00:15<08:31,  3.48s/it]\u001b[A\n",
            "Scraping rows:   4%|▍         | 6/152 [00:22<10:58,  4.51s/it]\u001b[A\n",
            "Scraping rows:   5%|▍         | 7/152 [00:32<15:40,  6.49s/it]\u001b[A\n",
            "Scraping rows:   5%|▌         | 8/152 [00:41<17:08,  7.14s/it]\u001b[A\n",
            "Scraping rows:   6%|▌         | 9/152 [00:48<17:17,  7.26s/it]\u001b[A\n",
            "Scraping rows:   7%|▋         | 10/152 [00:52<14:25,  6.10s/it]\u001b[A\n",
            "Scraping rows:   7%|▋         | 11/152 [01:02<17:37,  7.50s/it]\u001b[A\n",
            "Scraping rows:   8%|▊         | 12/152 [01:07<15:29,  6.64s/it]\u001b[A\n",
            "Scraping rows:   9%|▊         | 13/152 [01:18<18:06,  7.82s/it]\u001b[A\n",
            "Scraping rows:   9%|▉         | 14/152 [01:19<13:36,  5.92s/it]\u001b[A\n",
            "Scraping rows:  10%|▉         | 15/152 [01:24<12:31,  5.49s/it]\u001b[A\n",
            "Scraping rows:  11%|█         | 16/152 [01:29<12:26,  5.49s/it]\u001b[A\n",
            "Scraping rows:  11%|█         | 17/152 [01:34<11:41,  5.20s/it]\u001b[A\n",
            "Scraping rows:  12%|█▏        | 18/152 [01:42<13:50,  6.20s/it]\u001b[A\n",
            "Scraping rows:  12%|█▎        | 19/152 [01:48<13:16,  5.99s/it]\u001b[A\n",
            "Scraping rows:  13%|█▎        | 20/152 [01:56<14:50,  6.75s/it]\u001b[A\n",
            "Scraping rows:  14%|█▍        | 21/152 [02:03<14:36,  6.69s/it]\u001b[A\n",
            "Scraping rows:  14%|█▍        | 22/152 [02:12<16:20,  7.54s/it]\u001b[A\n",
            "Scraping rows:  15%|█▌        | 23/152 [02:16<13:37,  6.34s/it]\u001b[A\n",
            "Scraping rows:  16%|█▌        | 24/152 [02:23<14:15,  6.68s/it]\u001b[A\n",
            "Scraping rows:  16%|█▋        | 25/152 [02:31<14:40,  6.93s/it]\u001b[A\n",
            "Scraping rows:  17%|█▋        | 26/152 [02:35<13:02,  6.21s/it]\u001b[A\n",
            "Scraping rows:  18%|█▊        | 27/152 [02:41<12:28,  5.99s/it]\u001b[A\n",
            "Scraping rows:  18%|█▊        | 28/152 [02:51<15:10,  7.34s/it]\u001b[A\n",
            "Scraping rows:  19%|█▉        | 29/152 [03:01<16:23,  7.99s/it]\u001b[A\n",
            "Scraping rows:  20%|█▉        | 30/152 [03:06<14:43,  7.24s/it]\u001b[A\n",
            "Scraping rows:  20%|██        | 31/152 [03:14<14:48,  7.35s/it]\u001b[A\n",
            "Scraping rows:  21%|██        | 32/152 [03:15<11:10,  5.59s/it]\u001b[A\n",
            "Scraping rows:  22%|██▏       | 33/152 [03:24<12:48,  6.46s/it]\u001b[A\n",
            "Scraping rows:  22%|██▏       | 34/152 [03:32<13:57,  7.10s/it]\u001b[A\n",
            "Scraping rows:  23%|██▎       | 35/152 [03:35<11:11,  5.74s/it]\u001b[A\n",
            "Scraping rows:  24%|██▎       | 36/152 [03:41<11:31,  5.96s/it]\u001b[A\n",
            "Scraping rows:  24%|██▍       | 37/152 [03:44<09:26,  4.93s/it]\u001b[A\n",
            "Scraping rows:  25%|██▌       | 38/152 [03:51<10:48,  5.69s/it]\u001b[A\n",
            "Scraping rows:  26%|██▌       | 39/152 [03:59<11:43,  6.23s/it]\u001b[A\n",
            "Scraping rows:  26%|██▋       | 40/152 [04:08<13:27,  7.21s/it]\u001b[A\n",
            "Scraping rows:  27%|██▋       | 41/152 [04:15<12:59,  7.02s/it]\u001b[A\n",
            "Scraping rows:  28%|██▊       | 42/152 [04:17<09:50,  5.37s/it]\u001b[A\n",
            "Scraping rows:  28%|██▊       | 43/152 [04:21<09:16,  5.11s/it]\u001b[A\n",
            "Scraping rows:  29%|██▉       | 44/152 [04:28<09:56,  5.52s/it]\u001b[A\n",
            "Scraping rows:  30%|██▉       | 45/152 [04:38<12:31,  7.02s/it]\u001b[A\n",
            "Scraping rows:  30%|███       | 46/152 [04:45<12:09,  6.88s/it]\u001b[A\n",
            "Scraping rows:  31%|███       | 47/152 [04:53<12:53,  7.37s/it]\u001b[A\n",
            "Scraping rows:  32%|███▏      | 48/152 [05:04<14:26,  8.33s/it]\u001b[A\n",
            "Scraping rows:  32%|███▏      | 49/152 [05:07<11:47,  6.87s/it]\u001b[A\n",
            "Scraping rows:  33%|███▎      | 50/152 [05:17<13:00,  7.65s/it]\u001b[A\n",
            "Scraping rows:  34%|███▎      | 51/152 [05:25<13:19,  7.92s/it]\u001b[A\n",
            "Scraping rows:  34%|███▍      | 52/152 [05:34<13:28,  8.08s/it]\u001b[A\n",
            "Scraping rows:  35%|███▍      | 53/152 [05:43<14:02,  8.51s/it]\u001b[A\n",
            "Scraping rows:  36%|███▌      | 54/152 [05:50<12:54,  7.90s/it]\u001b[A\n",
            "Scraping rows:  36%|███▌      | 55/152 [05:54<11:08,  6.89s/it]\u001b[A\n",
            "Scraping rows:  37%|███▋      | 56/152 [06:00<10:20,  6.47s/it]\u001b[A\n",
            "Scraping rows:  38%|███▊      | 57/152 [06:03<08:49,  5.57s/it]\u001b[A\n",
            "Scraping rows:  38%|███▊      | 58/152 [06:07<07:47,  4.97s/it]\u001b[A\n",
            "Scraping rows:  39%|███▉      | 59/152 [06:14<08:53,  5.74s/it]\u001b[A\n",
            "Scraping rows:  39%|███▉      | 60/152 [06:18<07:47,  5.08s/it]\u001b[A\n",
            "Scraping rows:  40%|████      | 61/152 [06:28<10:10,  6.71s/it]\u001b[A\n",
            "Scraping rows:  41%|████      | 62/152 [06:32<08:36,  5.74s/it]\u001b[A\n",
            "Scraping rows:  41%|████▏     | 63/152 [06:42<10:38,  7.17s/it]\u001b[A\n",
            "Scraping rows:  42%|████▏     | 64/152 [06:45<08:26,  5.76s/it]\u001b[A\n",
            "Scraping rows:  43%|████▎     | 65/152 [06:47<06:55,  4.77s/it]\u001b[A\n",
            "Scraping rows:  43%|████▎     | 66/152 [06:50<05:50,  4.08s/it]\u001b[A\n",
            "Scraping rows:  44%|████▍     | 67/152 [06:53<05:33,  3.93s/it]\u001b[A\n",
            "Scraping rows:  45%|████▍     | 68/152 [06:56<04:53,  3.49s/it]\u001b[A\n",
            "Scraping rows:  45%|████▌     | 69/152 [07:06<07:44,  5.60s/it]\u001b[A\n",
            "Scraping rows:  46%|████▌     | 70/152 [07:14<08:25,  6.17s/it]\u001b[A\n",
            "Scraping rows:  47%|████▋     | 71/152 [07:16<06:52,  5.09s/it]\u001b[A\n",
            "Scraping rows:  47%|████▋     | 72/152 [07:26<08:35,  6.44s/it]\u001b[A\n",
            "Scraping rows:  48%|████▊     | 73/152 [07:36<10:06,  7.68s/it]\u001b[A\n",
            "Scraping rows:  49%|████▊     | 74/152 [07:46<10:41,  8.23s/it]\u001b[A\n",
            "Scraping rows:  49%|████▉     | 75/152 [07:52<09:53,  7.70s/it]\u001b[A\n",
            "Scraping rows:  50%|█████     | 76/152 [08:00<09:40,  7.64s/it]\u001b[A\n",
            "Scraping rows:  51%|█████     | 77/152 [08:05<08:45,  7.01s/it]\u001b[A\n",
            "Scraping rows:  51%|█████▏    | 78/152 [08:07<06:36,  5.36s/it]\u001b[A\n",
            "Scraping rows:  52%|█████▏    | 79/152 [08:16<08:02,  6.61s/it]\u001b[A\n",
            "Scraping rows:  53%|█████▎    | 80/152 [08:19<06:27,  5.38s/it]\u001b[A\n",
            "Scraping rows:  53%|█████▎    | 81/152 [08:26<07:06,  6.01s/it]\u001b[A\n",
            "Scraping rows:  54%|█████▍    | 82/152 [08:36<08:13,  7.05s/it]\u001b[A\n",
            "Scraping rows:  55%|█████▍    | 83/152 [08:45<08:37,  7.50s/it]\u001b[A\n",
            "Scraping rows:  55%|█████▌    | 84/152 [08:52<08:30,  7.51s/it]\u001b[A\n",
            "Scraping rows:  56%|█████▌    | 85/152 [09:00<08:23,  7.51s/it]\u001b[A\n",
            "Scraping rows:  57%|█████▋    | 86/152 [09:09<08:56,  8.12s/it]\u001b[A\n",
            "Scraping rows:  57%|█████▋    | 87/152 [09:14<07:37,  7.04s/it]\u001b[A\n",
            "Scraping rows:  58%|█████▊    | 88/152 [09:18<06:41,  6.27s/it]\u001b[A\n",
            "Scraping rows:  59%|█████▊    | 89/152 [09:23<06:01,  5.74s/it]\u001b[A\n",
            "Scraping rows:  59%|█████▉    | 90/152 [09:24<04:37,  4.48s/it]\u001b[A\n",
            "Scraping rows:  60%|█████▉    | 91/152 [09:27<03:57,  3.89s/it]\u001b[A\n",
            "Scraping rows:  61%|██████    | 92/152 [09:36<05:34,  5.58s/it]\u001b[A\n",
            "Scraping rows:  61%|██████    | 93/152 [09:45<06:21,  6.46s/it]\u001b[A\n",
            "Scraping rows:  62%|██████▏   | 94/152 [09:55<07:24,  7.67s/it]\u001b[A\n",
            "Scraping rows:  62%|██████▎   | 95/152 [09:58<05:49,  6.13s/it]\u001b[A\n",
            "Scraping rows:  63%|██████▎   | 96/152 [10:04<05:50,  6.27s/it]\u001b[A\n",
            "Scraping rows:  64%|██████▍   | 97/152 [10:07<04:41,  5.13s/it]\u001b[A\n",
            "Scraping rows:  64%|██████▍   | 98/152 [10:15<05:31,  6.13s/it]\u001b[A\n",
            "Scraping rows:  65%|██████▌   | 99/152 [10:25<06:18,  7.15s/it]\u001b[A\n",
            "Scraping rows:  66%|██████▌   | 100/152 [10:34<06:48,  7.86s/it]\u001b[A\n",
            "Scraping rows:  66%|██████▋   | 101/152 [10:36<05:06,  6.01s/it]\u001b[A\n",
            "Scraping rows:  67%|██████▋   | 102/152 [10:44<05:23,  6.48s/it]\u001b[A\n",
            "Scraping rows:  68%|██████▊   | 103/152 [10:49<05:03,  6.19s/it]\u001b[A\n",
            "Scraping rows:  68%|██████▊   | 104/152 [11:00<05:59,  7.50s/it]\u001b[A\n",
            "Scraping rows:  69%|██████▉   | 105/152 [11:05<05:23,  6.89s/it]\u001b[A\n",
            "Scraping rows:  70%|██████▉   | 106/152 [11:10<04:44,  6.18s/it]\u001b[A\n",
            "Scraping rows:  70%|███████   | 107/152 [11:11<03:35,  4.78s/it]\u001b[A\n",
            "Scraping rows:  71%|███████   | 108/152 [11:16<03:26,  4.70s/it]\u001b[A\n",
            "Scraping rows:  72%|███████▏  | 109/152 [11:19<03:06,  4.35s/it]\u001b[A\n",
            "Scraping rows:  72%|███████▏  | 110/152 [11:28<03:55,  5.60s/it]\u001b[A\n",
            "Scraping rows:  73%|███████▎  | 111/152 [11:38<04:49,  7.07s/it]\u001b[A\n",
            "Scraping rows:  74%|███████▎  | 112/152 [11:43<04:11,  6.30s/it]\u001b[A\n",
            "Scraping rows:  74%|███████▍  | 113/152 [11:45<03:20,  5.15s/it]\u001b[A\n",
            "Scraping rows:  75%|███████▌  | 114/152 [11:53<03:42,  5.86s/it]\u001b[A\n",
            "Scraping rows:  76%|███████▌  | 115/152 [11:56<03:10,  5.15s/it]\u001b[A\n",
            "Scraping rows:  76%|███████▋  | 116/152 [12:02<03:09,  5.27s/it]\u001b[A\n",
            "Scraping rows:  77%|███████▋  | 117/152 [12:11<03:48,  6.54s/it]\u001b[A\n",
            "Scraping rows:  78%|███████▊  | 118/152 [12:15<03:11,  5.63s/it]\u001b[A\n",
            "Scraping rows:  78%|███████▊  | 119/152 [12:22<03:24,  6.19s/it]\u001b[A\n",
            "Scraping rows:  79%|███████▉  | 120/152 [12:29<03:20,  6.27s/it]\u001b[A\n",
            "Scraping rows:  80%|███████▉  | 121/152 [12:37<03:35,  6.95s/it]\u001b[A\n",
            "Scraping rows:  80%|████████  | 122/152 [12:39<02:39,  5.31s/it]\u001b[A\n",
            "Scraping rows:  81%|████████  | 123/152 [12:41<02:09,  4.47s/it]\u001b[A\n",
            "Scraping rows:  82%|████████▏ | 124/152 [12:49<02:31,  5.39s/it]\u001b[A\n",
            "Scraping rows:  82%|████████▏ | 125/152 [12:53<02:18,  5.13s/it]\u001b[A\n",
            "Scraping rows:  83%|████████▎ | 126/152 [12:58<02:08,  4.94s/it]\u001b[A\n",
            "Scraping rows:  84%|████████▎ | 127/152 [13:03<02:07,  5.12s/it]\u001b[A\n",
            "Scraping rows:  84%|████████▍ | 128/152 [13:10<02:12,  5.52s/it]\u001b[A\n",
            "Scraping rows:  85%|████████▍ | 129/152 [13:17<02:20,  6.11s/it]\u001b[A\n",
            "Scraping rows:  86%|████████▌ | 130/152 [13:23<02:10,  5.94s/it]\u001b[A\n",
            "Scraping rows:  86%|████████▌ | 131/152 [13:33<02:34,  7.33s/it]\u001b[A\n",
            "Scraping rows:  87%|████████▋ | 132/152 [13:36<01:57,  5.88s/it]\u001b[A\n",
            "Scraping rows:  88%|████████▊ | 133/152 [13:41<01:49,  5.77s/it]\u001b[A\n",
            "Scraping rows:  88%|████████▊ | 134/152 [13:50<01:58,  6.61s/it]\u001b[A\n",
            "Scraping rows:  89%|████████▉ | 135/152 [13:56<01:51,  6.58s/it]\u001b[A\n",
            "Scraping rows:  89%|████████▉ | 136/152 [14:06<01:59,  7.46s/it]\u001b[A\n",
            "Scraping rows:  90%|█████████ | 137/152 [14:15<02:01,  8.08s/it]\u001b[A\n",
            "Scraping rows:  91%|█████████ | 138/152 [14:21<01:42,  7.33s/it]\u001b[A\n",
            "Scraping rows:  91%|█████████▏| 139/152 [14:27<01:28,  6.78s/it]\u001b[A\n",
            "Scraping rows:  92%|█████████▏| 140/152 [14:28<01:02,  5.19s/it]\u001b[A\n",
            "Scraping rows:  93%|█████████▎| 141/152 [14:38<01:11,  6.49s/it]\u001b[A\n",
            "Scraping rows:  93%|█████████▎| 142/152 [14:45<01:08,  6.80s/it]\u001b[A\n",
            "Scraping rows:  94%|█████████▍| 143/152 [14:51<00:57,  6.41s/it]\u001b[A\n",
            "Scraping rows:  95%|█████████▍| 144/152 [14:59<00:56,  7.07s/it]\u001b[A\n",
            "Scraping rows:  95%|█████████▌| 145/152 [15:07<00:50,  7.21s/it]\u001b[A\n",
            "Scraping rows:  96%|█████████▌| 146/152 [15:15<00:45,  7.59s/it]\u001b[A\n",
            "Scraping rows:  97%|█████████▋| 147/152 [15:24<00:39,  7.89s/it]\u001b[A\n",
            "Scraping rows:  97%|█████████▋| 148/152 [15:28<00:27,  6.88s/it]\u001b[A\n",
            "Scraping rows:  98%|█████████▊| 149/152 [15:33<00:18,  6.17s/it]\u001b[A\n",
            "Scraping rows:  99%|█████████▊| 150/152 [15:40<00:13,  6.58s/it]\u001b[A\n",
            "Scraping rows:  99%|█████████▉| 151/152 [15:43<00:05,  5.36s/it]\u001b[A\n",
            "Scraping rows: 100%|██████████| 152/152 [15:46<00:00,  6.23s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# URL to scrape\n",
        "url = 'https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/appendix-l--events-to-monitor'\n",
        "\n",
        "# Fetch the page\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all tables\n",
        "tables = soup.find_all('table')\n",
        "\n",
        "# Initialize an empty list to hold all rows from all tables\n",
        "all_rows = []\n",
        "\n",
        "# Iterate over each table\n",
        "for table in tables:\n",
        "    # Find all rows in the current table\n",
        "    rows = table.find_all('tr')\n",
        "    # Skip the header row\n",
        "    rows = rows[1:]\n",
        "    # Extract data from each row\n",
        "    for row in rows:\n",
        "        cols = row.find_all('td')\n",
        "        if cols:\n",
        "            current_id = cols[0].text.strip()\n",
        "            legacy_id = cols[1].text.strip()\n",
        "            criticality = cols[2].text.strip()\n",
        "            summary = cols[3].text.strip()\n",
        "            # Normalize Legacy_Event_ID\n",
        "            legacy_ids = re.split(r',\\s*', legacy_id)\n",
        "            expanded_ids = []\n",
        "            for id_range in legacy_ids:\n",
        "                if '-' in id_range:\n",
        "                    start, end = map(int, id_range.split('-'))\n",
        "                    expanded_ids.extend(map(str, range(start, end + 1)))\n",
        "                else:\n",
        "                    expanded_ids.append(id_range)\n",
        "            normalized_legacy_ids = ','.join(expanded_ids)\n",
        "            all_rows.append([current_id, normalized_legacy_ids, criticality, summary])\n",
        "\n",
        "# Create a DataFrame\n",
        "new_df = pd.DataFrame(all_rows, columns=['Current_Event_ID', 'Legacy_Event_ID', 'Potential_Criticality', 'Event_Summary'])"
      ],
      "metadata": {
        "id": "ZLOMbzVq-8G9"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing CSV\n",
        "existing_df = pd.read_csv('detailed_events.csv')\n",
        "\n",
        "# Convert the 'Current_Event_ID' column in 'new_df' to int64\n",
        "new_df['Current_Event_ID'] = pd.to_numeric(new_df['Current_Event_ID'], errors='coerce')\n",
        "\n",
        "# Merge the new data based on Event_ID using left join\n",
        "merged_df = existing_df.merge(new_df, left_on='Event_ID', right_on='Current_Event_ID', how='left')\n",
        "\n",
        "# Drop the extra columns if not needed\n",
        "merged_df.drop(columns=['Current_Event_ID'], inplace=True)\n",
        "\n",
        "# Rename the 'Event_Summary_y' column to 'Event_Summary' if it exists\n",
        "if 'Event_Summary_y' in merged_df.columns:\n",
        "    merged_df.rename(columns={'Event_Summary_y': 'Event_Summary'}, inplace=True)\n",
        "\n",
        "# Drop the 'Event_Summary_x' column if it exists\n",
        "if 'Event_Summary_x' in merged_df.columns:\n",
        "    merged_df.drop(columns=['Event_Summary_x'], inplace=True)\n",
        "\n",
        "# Remove '000' from the 'Legacy_Event_ID' column\n",
        "merged_df['Legacy_Event_ID'] = merged_df['Legacy_Event_ID'].apply(lambda x: ','.join([id for id in str(x).split(',') if id.strip() != '000']))\n",
        "\n",
        "# Save the updated DataFrame\n",
        "merged_df.to_csv('updated_detailed_events.csv', index=False)"
      ],
      "metadata": {
        "id": "lh9GJO73AT6l"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Load the existing CSV\n",
        "df = pd.read_csv('updated_detailed_events.csv')\n",
        "\n",
        "# Ensure the 'Operating Systems' column is read as a string\n",
        "df['Operating Systems'] = df['Operating Systems'].astype(str)\n",
        "\n",
        "# Remove new lines from the 'Operating Systems' column and replace values\n",
        "df['Operating Systems'] = df['Operating Systems'].apply(lambda x: x.replace('\\\\r\\\\n', ' ')\n",
        "                                                                   .replace('2022', 'Windows Server 2022')\n",
        "                                                                   .replace('7', 'Windows 7')\n",
        "                                                                   .replace('8.1', 'Windows 8.1')\n",
        "                                                                   .replace('Windows 2003', 'Windows Server 2003')\n",
        "                                                                   .replace('Windows 2016', 'Windows Server 2016')\n",
        "                                                                   .replace('10', 'Windows 10')\n",
        "                                                                   .replace('XP', 'Windows XP'))\n",
        "\n",
        "# Split the 'Operating Systems' string using regular expressions and join with commas\n",
        "df['Operating Systems'] = df['Operating Systems'].apply(lambda x: ', '.join(re.findall(r'Windows (?:Server )?(?:2008 R2|2012 R2|2000|2003|2016|2019|2022|7|8\\.1|10|XP)', x)))\n",
        "\n",
        "# Remove undesired characters from all columns\n",
        "df = df.applymap(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', str(x)) if isinstance(x, str) else x)\n",
        "\n",
        "# Remove the specific text pattern from the 'Description' column\n",
        "df['Description'] = df['Description'].apply(lambda x: re.sub(r\"I haven't been able to produce this event\\. Have you\\? If so, please start a discussion \\(see above\\) and post a sample along with any comments you may have! Don't forget to sanitize any private information\\.\", '', str(x)))\n",
        "\n",
        "# Drop rows where all values are NaN\n",
        "df.dropna(how='all', inplace=True)\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df.to_csv('updated_detailed_events.csv', index=False)"
      ],
      "metadata": {
        "id": "pAAsSazNLRJW"
      },
      "execution_count": 79,
      "outputs": []
    }
  ]
}