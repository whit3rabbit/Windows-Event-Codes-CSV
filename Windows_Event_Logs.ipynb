{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install google-colab-selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1pwXVzoVFej",
        "outputId": "2d15b295-d593-4ca5-e605-0be2a8e7ab8f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-colab-selenium\n",
            "  Downloading google_colab_selenium-1.0.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting selenium (from google-colab-selenium)\n",
            "  Downloading selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium->google-colab-selenium)\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->google-colab-selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (25.1.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium->google-colab-selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium->google-colab-selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->google-colab-selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google-colab-selenium) (0.14.0)\n",
            "Downloading google_colab_selenium-1.0.14-py3-none-any.whl (8.2 kB)\n",
            "Downloading selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium, google-colab-selenium\n",
            "Successfully installed google-colab-selenium-1.0.14 outcome-1.3.0.post0 selenium-4.28.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "import google_colab_selenium as gs\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "\n",
        "# Base URL and headers remain the same\n",
        "base_url = 'https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/'\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "}\n",
        "\n",
        "def scrape_event_details(event_url):\n",
        "    # Event details scraping function remains the same\n",
        "    response = requests.get(event_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    os_label = soup.find('span', id='ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_lblOSorSourceLabel')\n",
        "    if os_label:\n",
        "        os_text = os_label.find_next('td').text.strip() if os_label.find_next('td') else 'Not available'\n",
        "        os_data = re.split(r'\\s+and\\s+', os_text)\n",
        "    else:\n",
        "        os_data = ['Not available']\n",
        "\n",
        "    category_span = soup.find('span', id='ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_categories')\n",
        "    category_data = category_span.text.strip() if category_span else 'Not available'\n",
        "\n",
        "    type_row = soup.find('tr', id='ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_trType')\n",
        "    type_data = type_row.find_all('td')[1].text.strip() if type_row and len(type_row.find_all('td')) > 1 else 'Not available'\n",
        "\n",
        "    description = soup.find('ul').find_next_sibling('p')\n",
        "    description = description.text.strip() if description else 'No description available'\n",
        "\n",
        "    event_example = soup.find('p', class_='EventExample')\n",
        "    event_example = event_example.text.strip() if event_example else 'No example available'\n",
        "\n",
        "    return os_data, category_data, type_data, description, event_example\n",
        "\n",
        "def handle_cookie_consent(driver):\n",
        "    try:\n",
        "        # Try to remove the cookie consent div using JavaScript\n",
        "        driver.execute_script(\"\"\"\n",
        "            var cookieConsent = document.getElementById('ctl00_ctl00_ctl00_ctl00_CookieConsent');\n",
        "            if(cookieConsent) {\n",
        "                cookieConsent.parentNode.removeChild(cookieConsent);\n",
        "            }\n",
        "        \"\"\")\n",
        "        time.sleep(1)  # Brief pause to let the DOM update\n",
        "    except Exception as e:\n",
        "        print(f\"Error handling cookie consent: {str(e)}\")\n",
        "\n",
        "    # Also remove any potential overlay elements\n",
        "    driver.execute_script(\"\"\"\n",
        "        var elements = document.getElementsByTagName('div');\n",
        "        for(var i = 0; i < elements.length; i++) {\n",
        "            if(elements[i].style.zIndex > 1000) {\n",
        "                elements[i].parentNode.removeChild(elements[i]);\n",
        "            }\n",
        "        }\n",
        "    \"\"\")\n",
        "\n",
        "def main():\n",
        "    driver = gs.Chrome()\n",
        "    try:\n",
        "        # Navigate to the webpage\n",
        "        driver.get('https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/default.aspx')\n",
        "\n",
        "        # Handle cookie consent first\n",
        "        handle_cookie_consent(driver)\n",
        "\n",
        "        # Click the radio button using JavaScript\n",
        "        driver.execute_script(\"\"\"\n",
        "            var radioBtn = document.getElementById('ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_whichEvents_0');\n",
        "            if(radioBtn) {\n",
        "                radioBtn.click();\n",
        "            }\n",
        "        \"\"\")\n",
        "\n",
        "        # Wait for the page to update\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Get the updated page source\n",
        "        updated_page_source = driver.page_source\n",
        "        soup = BeautifulSoup(updated_page_source, 'html.parser')\n",
        "\n",
        "        table = soup.find('table', id='ctl00_ctl00_ctl00_ctl00_Content_Content_Content_Content_GridView1')\n",
        "        rows = table.find_all('tr')[1:]\n",
        "\n",
        "        data = []\n",
        "        existing_data = []\n",
        "\n",
        "        if os.path.exists('detailed_events.csv'):\n",
        "            existing_df = pd.read_csv('detailed_events.csv')\n",
        "            existing_data = existing_df['Event_ID'].astype(str).tolist()\n",
        "\n",
        "        rows_to_process = [row for row in rows if row.find_all('td')[1].text.strip() not in existing_data]\n",
        "\n",
        "        progress_bar = tqdm(total=len(rows_to_process), desc=\"Scraping rows\", dynamic_ncols=True)\n",
        "\n",
        "        for row in rows_to_process:\n",
        "            cols = row.find_all('td')\n",
        "            if cols:\n",
        "                source = cols[0].text.strip()\n",
        "                event_id = cols[1].text.strip()\n",
        "                event_summary = cols[2].text.strip()\n",
        "                event_url = base_url + cols[2].find('a')['href']\n",
        "                os_data, category_data, type_data, description, event_example = scrape_event_details(event_url)\n",
        "                data.append([source, event_id, event_summary, event_url, os_data, category_data, type_data, description, event_example])\n",
        "                time.sleep(random.randint(1, 3))  # Reduced sleep time\n",
        "                progress_bar.update(1)\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        df = pd.DataFrame(data, columns=['Source', 'Event_ID', 'Event Summary', 'URL', 'Operating Systems',\n",
        "                                       'Category and Subcategory', 'Type', 'Description', 'Event Example'])\n",
        "\n",
        "        if os.path.exists('detailed_events.csv'):\n",
        "            existing_df = pd.read_csv('detailed_events.csv')\n",
        "            combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
        "            combined_df.to_csv('detailed_events.csv', index=False)\n",
        "        else:\n",
        "            df.to_csv('detailed_events.csv', index=False)\n",
        "\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "loM1iX-r7iWl",
        "outputId": "1324a7bd-4511-4649-b96e-a7f2b9f10cc6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"7f153afc-31e5-48d7-8d36-bfc4de4c9d84-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"7f153afc-31e5-48d7-8d36-bfc4de4c9d84-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"7f153afc-31e5-48d7-8d36-bfc4de4c9d84-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"7f153afc-31e5-48d7-8d36-bfc4de4c9d84-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping rows: 100%|██████████| 573/573 [23:17<00:00,  2.44s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# URL to scrape\n",
        "url = 'https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/plan/appendix-l--events-to-monitor'\n",
        "\n",
        "# Fetch the page\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all tables\n",
        "tables = soup.find_all('table')\n",
        "\n",
        "# Initialize an empty list to hold all rows from all tables\n",
        "all_rows = []\n",
        "\n",
        "# Iterate over each table\n",
        "for table in tables:\n",
        "    # Find all rows in the current table\n",
        "    rows = table.find_all('tr')\n",
        "    # Skip the header row\n",
        "    rows = rows[1:]\n",
        "    # Extract data from each row\n",
        "    for row in rows:\n",
        "        cols = row.find_all('td')\n",
        "        if cols:\n",
        "            current_id = cols[0].text.strip()\n",
        "            legacy_id = cols[1].text.strip()\n",
        "            criticality = cols[2].text.strip()\n",
        "            summary = cols[3].text.strip()\n",
        "            # Normalize Legacy_Event_ID\n",
        "            legacy_ids = re.split(r',\\s*', legacy_id)\n",
        "            expanded_ids = []\n",
        "            for id_range in legacy_ids:\n",
        "                if '-' in id_range:\n",
        "                    start, end = map(int, id_range.split('-'))\n",
        "                    expanded_ids.extend(map(str, range(start, end + 1)))\n",
        "                else:\n",
        "                    expanded_ids.append(id_range)\n",
        "            normalized_legacy_ids = ','.join(expanded_ids)\n",
        "            all_rows.append([current_id, normalized_legacy_ids, criticality, summary])\n",
        "\n",
        "# Create a DataFrame\n",
        "new_df = pd.DataFrame(all_rows, columns=['Current_Event_ID', 'Legacy_Event_ID', 'Potential_Criticality', 'Event_Summary'])"
      ],
      "metadata": {
        "id": "ZLOMbzVq-8G9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing CSV\n",
        "existing_df = pd.read_csv('detailed_events.csv')\n",
        "\n",
        "# Convert the 'Current_Event_ID' column in 'new_df' to int64\n",
        "new_df['Current_Event_ID'] = pd.to_numeric(new_df['Current_Event_ID'], errors='coerce')\n",
        "\n",
        "# Merge the new data based on Event_ID using left join\n",
        "merged_df = existing_df.merge(new_df, left_on='Event_ID', right_on='Current_Event_ID', how='left')\n",
        "\n",
        "# Drop the extra columns if not needed\n",
        "merged_df.drop(columns=['Current_Event_ID'], inplace=True)\n",
        "\n",
        "# Rename the 'Event_Summary_y' column to 'Event_Summary' if it exists\n",
        "if 'Event_Summary_y' in merged_df.columns:\n",
        "    merged_df.rename(columns={'Event_Summary_y': 'Event_Summary'}, inplace=True)\n",
        "\n",
        "# Drop the 'Event_Summary_x' column if it exists\n",
        "if 'Event_Summary_x' in merged_df.columns:\n",
        "    merged_df.drop(columns=['Event_Summary_x'], inplace=True)\n",
        "\n",
        "# Remove '000' from the 'Legacy_Event_ID' column\n",
        "merged_df['Legacy_Event_ID'] = merged_df['Legacy_Event_ID'].apply(lambda x: ','.join([id for id in str(x).split(',') if id.strip() != '000']))\n",
        "\n",
        "# Save the updated DataFrame\n",
        "merged_df.to_csv('updated_detailed_events.csv', index=False)"
      ],
      "metadata": {
        "id": "lh9GJO73AT6l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Load the existing CSV\n",
        "df = pd.read_csv('updated_detailed_events.csv')\n",
        "\n",
        "# Ensure the 'Operating Systems' column is read as a string\n",
        "df['Operating Systems'] = df['Operating Systems'].astype(str)\n",
        "\n",
        "# Remove new lines from the 'Operating Systems' column and replace values\n",
        "df['Operating Systems'] = df['Operating Systems'].apply(lambda x: x.replace('\\\\r\\\\n', ' ')\n",
        "                                                                   .replace('2022', 'Windows Server 2022')\n",
        "                                                                   .replace('7', 'Windows 7')\n",
        "                                                                   .replace('8.1', 'Windows 8.1')\n",
        "                                                                   .replace('Windows 2003', 'Windows Server 2003')\n",
        "                                                                   .replace('Windows 2016', 'Windows Server 2016')\n",
        "                                                                   .replace('10', 'Windows 10')\n",
        "                                                                   .replace('XP', 'Windows XP'))\n",
        "\n",
        "# Split the 'Operating Systems' string using regular expressions and join with commas\n",
        "df['Operating Systems'] = df['Operating Systems'].apply(lambda x: ', '.join(re.findall(r'Windows (?:Server )?(?:2008 R2|2012 R2|2000|2003|2016|2019|2022|7|8\\.1|10|XP)', x)))\n",
        "\n",
        "# Remove undesired characters from all columns\n",
        "df = df.applymap(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', str(x)) if isinstance(x, str) else x)\n",
        "\n",
        "# Remove the specific text pattern from the 'Description' column\n",
        "df['Description'] = df['Description'].apply(lambda x: re.sub(r\"I haven't been able to produce this event\\. Have you\\? If so, please start a discussion \\(see above\\) and post a sample along with any comments you may have! Don't forget to sanitize any private information\\.\", '', str(x)))\n",
        "\n",
        "# Drop rows where all values are NaN\n",
        "df.dropna(how='all', inplace=True)\n",
        "\n",
        "# Save the updated DataFrame\n",
        "df.to_csv('updated_detailed_events.csv', index=False)"
      ],
      "metadata": {
        "id": "pAAsSazNLRJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927fff0d-f0c9-46f9-bd22-bbf20b221dc1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-ba6426bf1c27>:23: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', str(x)) if isinstance(x, str) else x)\n"
          ]
        }
      ]
    }
  ]
}